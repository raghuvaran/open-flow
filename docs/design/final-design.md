# OpenFlow — Final Design Document

**Project:** Local-first, system-wide AI voice dictation for macOS
**Status:** Implemented & Shipping (v0.1.0)
**Date:** February 2026
**License:** MIT

---

## 1. Product Vision

A system-wide, always-on voice dictation layer that runs 100% on-device. Press a hotkey, speak naturally, and polished text appears at the cursor in any application. No cloud APIs, no subscriptions, no data leaving the machine.

The 15 MB `.app` auto-downloads ~2.1 GB of models on first launch and requires zero configuration. No Homebrew, no terminal, no restarts.

### 1.1 What We Ship

| Capability | Status |
|---|---|
| Universal text injection (any app with a text field) | ✅ Shipping |
| Filler removal & grammar polish via local LLM | ✅ Shipping |
| Self-correction understanding ("no wait, 4pm" → keeps correction) | ✅ Shipping |
| Context-aware tone (email vs Slack vs code) | ✅ Shipping |
| Push-to-talk (walkie-talkie) + toggle modes | ✅ Shipping |
| Voice commands ("new paragraph", "scratch that") | ✅ Shipping |
| Auto-download models + llama-server on first launch | ✅ Shipping |
| Transparent pill overlay with waveform visualization | ✅ Shipping |
| Menu bar tray (polish toggle, mic selector, walkie mode) | ✅ Shipping |
| Smart contextual hints based on active app | ✅ Shipping |
| Zero-restart permission flow (accessibility, mic) | ✅ Shipping |

### 1.2 Measured Performance

```
VAD (energy-based):     < 1ms per 30ms frame
ASR (whisper-base, CPU): 300ms per 1-5s segment
LLM polish (Qwen 3B):   400-800ms (Metal GPU via llama-server)
Text injection (CGEvent): ~150ms
────────────────────────────────────────
Total pipeline:          1.0-1.5s per utterance
```

---

## 2. Design Philosophy

### Core Principle: Never Make the User Leave

OpenFlow is a background utility. The moment we force the user to think about OpenFlow instead of their actual work, we've failed.

### Rule 1: The Pill Is the Interface

The pill is always visible, always on top, and the single source of truth. No popups, no modals, no separate windows, no notifications. If we can't say it in the pill, we shouldn't say it. Show the pill without stealing focus from the active app.

### Rule 2: Guide In Place, Not In Docs

Assume the user will never read documentation. Use progressive disclosure — show the next step, not all steps. Auto-detect completion (poll permissions, check files). The transition from guidance to "Ready" is the reward.

Example: `⚠ Enable Accessibility →` (click) → `Click + → add OpenFlow → toggle on` (hint) → `Ready` (auto-detected within 3s).

### Rule 3: Zero Restarts, Zero External Steps

First launch to working dictation without closing the app, reopening it, or running a terminal command. Download everything automatically. Hot-reload permissions. Degrade gracefully. Never error with "install X."

### Rule 4: Earn Attention, Don't Spend It

Every character in the pill competes with the user's actual work. Status text should confirm what just happened, not entertain. Smart hints are generated by the LLM but only shown when idle, never during dictation, and dismissed instantly on interaction.

### Decision Framework

1. Can the pill handle it? → Do it in the pill.
2. Does the user need to act? → Show the next step, auto-detect when done.
3. Does it require a restart? → Hot-reload it.
4. Does it require an external tool? → Bundle it or download it.
5. Can we skip it entirely? → Degrade gracefully instead.

---

## 3. Architecture

### 3.1 Data Flow

```
Mic (cpal, device default) → Resample 16kHz mono → Energy VAD → Chunker
    → Whisper ASR (~300ms) → LLM Polish via llama-server (~500ms)
    → CGEvent Cmd+V paste → Active app
```

### 3.2 Process Architecture

```
┌─────────────────────────────────────────────────────┐
│  OpenFlow.app (Tauri 2 + Svelte)                    │
│                                                      │
│  Main thread: Tauri event loop + UI                  │
│  Audio thread: cpal capture + resample + VAD + chunk │
│  Tauri runtime: Orchestrator + ASR (spawn_blocking)  │
│  Background: Accessibility poll, hint generator      │
│                                                      │
│  Child process: llama-server (port 8384, Metal GPU)  │
└─────────────────────────────────────────────────────┘
```

**Why two processes:** whisper-rs and llama-cpp both vendor ggml. Linking both into one binary causes a fatal symbol collision (SIGABRT in Metal backend registry). llama-server runs out-of-process, eliminating the collision while keeping everything local.

**Why two runtimes:** cpal's `Stream` is `!Send` on macOS. It cannot live on tokio's multi-threaded runtime. The audio pipeline runs on a dedicated `std::thread` with its own `current_thread` tokio runtime. The orchestrator runs on Tauri's multi-threaded runtime because `spawn_blocking` tasks survive the audio thread stopping (critical for walkie-talkie mode where the audio thread is killed before the final segment is processed).

### 3.3 File Layout

```
openflow/
├── src/routes/+page.svelte       # Pill UI: waveform, controls, events
├── src-tauri/src/
│   ├── lib.rs                    # App setup, tray, commands, audio loop
│   ├── audio/
│   │   ├── capture.rs            # cpal mic → resample → mono, explicit Drop
│   │   ├── vad.rs                # Silero wrapper (falls back to energy-based)
│   │   └── chunker.rs            # Segment detection, 60s buffer cap
│   ├── asr/engine.rs             # whisper.cpp via whisper-rs
│   ├── polish/
│   │   ├── engine.rs             # llama-server lifecycle + HTTP client
│   │   └── prompt.rs             # Context-aware system prompt builder
│   ├── pipeline/orchestrator.rs  # ASR → polish → inject, app usage tracking
│   ├── inject/
│   │   ├── clipboard.rs          # CGEvent Cmd+V, accessibility check
│   │   └── context.rs            # Active app detection via osascript
│   ├── models/download.rs        # Auto-download from HuggingFace/GitHub
│   ├── db/
│   │   ├── schema.rs             # SQLite schema (settings, history, hints)
│   │   ├── hints.rs              # Hint cache CRUD
│   │   └── settings.rs           # Key-value settings
│   └── config.rs                 # Paths and defaults
└── tauri.conf.json
```

---

## 4. First Launch Experience

The entire first-launch flow happens without the user leaving the app or restarting:

```
1. App opens → pill shows "Downloading models..."
2. Downloads (with progress in pill):
   - silero_vad.onnx (2 MB, ~1s)
   - ggml-base.bin (141 MB, ~9s)
   - qwen2.5-3b-instruct-q4_k_m.gguf (2 GB, ~2 min)
   - llama-server + dylibs (29 MB tarball, ~3s)
3. Pill shows "Loading models..." (~2s)
4. Pill shows "Ready"
5. If accessibility not granted:
   - Pill shows "⚠ Enable Accessibility →" (clickable, amber)
   - Click → System Settings opens to Accessibility pane
   - Pill shows "Click + → add OpenFlow → toggle on" (hint)
   - User grants permission → pill auto-clears within 3s
6. Mic permission: macOS prompts on first listen, works immediately after grant
```

### 4.1 Model Downloads

All models stored in `~/Library/Application Support/openflow/models/`:

| Asset | Size | Source | Purpose |
|-------|------|--------|---------|
| `silero_vad.onnx` | 2 MB | GitHub (snakers4) | Voice activity detection |
| `ggml-base.bin` | 141 MB | HuggingFace (ggerganov) | Whisper ASR |
| `qwen2.5-3b-instruct-q4_k_m.gguf` | 2 GB | HuggingFace (Qwen) | LLM polish |
| `llama-server` + dylibs | 29 MB | GitHub (ggml-org, b8123) | LLM inference server |

Downloads use `.part` temp files — interrupted downloads don't leave corrupt files. Existing files are skipped. The llama-server tarball is extracted, extra binaries removed, and `com.apple.quarantine`/`com.apple.provenance` xattrs stripped so macOS doesn't block execution.

### 4.2 Permission Flow

**Accessibility:** Backend polls `AXIsProcessTrustedWithOptions` every 3 seconds. Emits `accessibility_missing` until granted, then `accessibility_granted` (stops polling). The pill is the guide — clickable to open System Settings, with progressive hints.

**Microphone:** macOS prompts automatically on first `cpal` stream open. No custom handling needed — if denied, next `start_listening` call retries.

**Key principle:** No permission requires a restart. The app hot-reloads all permission states.

---

## 5. Audio Pipeline

```
Mic (device default, typically 48kHz stereo)
  → cpal callback: resample to 16kHz mono (linear interpolation)
  → mpsc::UnboundedSender<Vec<f32>>
  → Energy-based VAD (RMS > 0.01 per 480-sample frame)
  → Chunker (accumulate speech, dispatch on 700ms silence)
  → Segments > 4800 samples (0.3s) → Orchestrator
```

### 5.1 Key Decisions

**Resample in callback, not in config:** MacBook mics don't support 16kHz directly. Forcing it via cpal config fails. Always use `device.default_input_config()` and resample in software.

**Energy-based VAD over Silero:** Silero VAD requires `libonnxruntime.dylib` which isn't bundled. The `ort` crate panics at runtime if missing. Energy-based detection (RMS threshold) works well for normal speech. Silero is wrapped in `catch_unwind` as an optional upgrade.

**mpsc over ring buffer:** Simpler, no sizing issues. The cpal callback sends resampled chunks directly. The consumer loop processes them in a `tokio::select!` alongside the stop signal.

**60-second buffer cap:** Chunker auto-dispatches if continuous speech exceeds 60s (960K samples, ~3.8MB). Prevents unbounded memory growth in noisy environments.

### 5.2 Mic Stream Lifecycle

`AudioCapture` has an explicit `stop()` method and a `Drop` impl that releases the cpal `Stream`. This ensures the macOS mic indicator (orange dot) disappears when not listening. The `stop()` is called before `chunker.flush()` on walkie-talkie release.

---

## 6. ASR Engine

**Stack:** whisper.cpp via `whisper-rs 0.13`, base model (141 MB), CPU-only.

**Why CPU-only:** Metal for whisper is safe now that llama-cpp is out-of-process, but hasn't been re-enabled yet. CPU performance (~300ms for base model) is adequate. Re-enabling Metal would cut this to ~100ms.

**Output filtering:** Whisper outputs artifacts like `[no speech detected]`, `[BLANK_AUDIO]`, `(music)` for non-speech. Any output starting with `[` or `(` is discarded before reaching the polish step.

**Minimum segment length:** 4800 samples (0.3s). Shorter segments are noise bursts that produce garbage ASR output.

---

## 7. LLM Polish Engine

**Stack:** Qwen 2.5 3B Instruct (Q4_K_M, 2 GB) via llama-server on localhost:8384.

### 7.1 Server Lifecycle

`PolishEngine::new()` spawns `llama-server` as a child process with `-ngl 99` (full Metal GPU offload). Waits up to 30s for `/health` endpoint. `Drop` impl kills the child process.

**Orphan protection:** On startup, `ensure_server()` runs `lsof -ti tcp:8384 | xargs kill -9` to clean up any orphaned server from a previous crash. This prevents "port already in use" failures.

**Binary resolution order:** Bundled copy in models dir → `/opt/homebrew/bin/llama-server` → `/usr/local/bin/llama-server` → PATH lookup.

### 7.2 Context-Aware Prompting

The system prompt includes:
- Active app name and category (email, slack, code, terminal, notes, default)
- Tone directive per category ("Professional, concise" for email, "Casual" for Slack)
- Personal dictionary entries
- Instructions for filler removal, self-correction detection, voice commands

### 7.3 LLM Priority

The llama-server is shared between dictation polish and background hint generation. **Dictation always has priority.** Hint generation only runs when idle 5+ minutes and never interrupts a polish request.

---

## 8. Text Injection

**Stack:** `arboard` (clipboard) + `core-graphics` CGEvent API (Cmd+V simulation).

### 8.1 Why CGEvent Over Alternatives

| Approach | Problem |
|----------|---------|
| `enigo` crate | Crashes on macOS from non-main thread (`dispatch_assert_queue_fail`) |
| `osascript` keystroke | Breaks when launched from Spotlight (different app identity, no Accessibility permission) |
| CGEvent API | Works from any thread, permission tied to the `.app` bundle directly |

### 8.2 Injection Flow

```
1. Save current clipboard (arboard)
2. Set clipboard to polished text
3. Sleep 50ms (let pasteboard sync)
4. CGEvent: key down 'v' + Command flag, key up 'v'
5. Sleep 100ms (let paste complete)
6. Restore original clipboard
```

### 8.3 Accessibility Check

`AXIsProcessTrustedWithOptions` FFI call. If not trusted, the pill shows a clickable warning that opens System Settings directly to the Accessibility pane. Backend polls every 3s until granted.

---

## 9. Walkie-Talkie Mode

Optional push-to-talk mode, toggled from tray, persisted to SQLite.

**Normal mode:** `Ctrl+Shift+Space` toggles listening. VAD silence detection auto-dispatches segments.

**Walkie-talkie mode:** Hold `Ctrl+Shift+Space` to record, release to process. Audio loop feeds chunker with `is_speech=true` always (prevents auto-dispatch). On key release: `capture.stop()` → `chunker.flush()` → full segment sent to pipeline.

**Critical architecture detail:** The orchestrator must run on Tauri's multi-threaded runtime, NOT the audio thread's `current_thread` runtime. When the audio thread stops (on key release), its runtime shuts down. If the orchestrator's `spawn_blocking` tasks were on that runtime, they'd be killed before processing the flushed segment.

---

## 10. Smart Hints

LLM-generated contextual tips shown in the pill when idle. Makes the app feel alive without being annoying.

### 10.1 How It Works

1. Each dictation records the active app name to `injection_history`
2. Background task wakes every 5 minutes
3. If idle 5+ min: queries top 5 most-used apps from last 7 days
4. For apps without today's hint: sends one batch prompt to Qwen
5. Caches results in `hint_cache` table (keyed by app + date)
6. Frontend polls `get_hint` every 30s when idle
7. Shows cached hint in pill as subtle italic text
8. Hint dismissed instantly on listen start

### 10.2 Constraints

- Never generates during dictation (LLM pipeline has priority)
- Only generates when idle 5+ minutes
- Max 6 words per hint
- Cached daily — zero LLM cost on display
- Falls back to "Ready" if no cache for current app

---

## 11. UI/UX Details

### 11.1 The Pill

Transparent, pill-shaped overlay. Always on top, draggable, position persisted to SQLite.

- `cursor: grab` on pill body, `pointer` on buttons, `default` on label text
- Close button (✕) appears on hover only, uses `pointerenter/leave` + `document.pointerleave` fallback for reliable hover detection on transparent windows
- Close = stop listening + hide window (not quit)
- Show = `window.show()` without `setFocus()` (never steals focus)

### 11.2 States

| State | Pill Appearance |
|-------|----------------|
| Loading | Spinner + "Loading models..." |
| Downloading | Spinner + "Whisper Base 80/141 MB (57%)" |
| Ready | Mic icon + "Ready" (or hint text) |
| Listening | Waveform bars (green) + "Listening" |
| Processing | Blue dot + "Processing" |
| Access warning | "⚠ Enable Accessibility →" (amber, clickable) |
| Access hint | "Click + → add OpenFlow → toggle on" (italic) |

### 11.3 Tray Menu

Show OpenFlow, AI Polish (checkbox), Walkie-Talkie Mode (checkbox), Start on Login (checkbox), Microphone (submenu with device list), Quit OpenFlow.

### 11.4 LSUIElement

`Info.plist` has `LSUIElement = true` — app doesn't appear in Dock or Cmd+Tab. Tray-only. After updating the `.app` bundle, run `lsregister -f /Applications/OpenFlow.app` to flush macOS LaunchServices cache.

---

## 12. Persistence

**SQLite** at `~/Library/Application Support/openflow/openflow.db`.

Key tables:
- `settings` — key-value pairs (walkie mode, autostart, mic selection)
- `injection_history` — raw + polished text, app name, timestamp (for hint generation)
- `hint_cache` — app name → hint text, generated date
- `personal_dict` — spoken form → written form
- `app_tones` — bundle ID → category + tone directive

**Logging:** `tracing_subscriber::fmt` writes to stderr only. When launched normally (LSUIElement), stderr is discarded. Zero disk usage from logging.

---

## 13. Build & Distribution

```bash
cd openflow
npm install
MACOSX_DEPLOYMENT_TARGET=11.0 npm run tauri build
cp -R src-tauri/target/release/bundle/macos/OpenFlow.app /Applications/
```

**App bundle:** ~15 MB. Models + llama-server downloaded on first launch (~2.1 GB).

**`MACOSX_DEPLOYMENT_TARGET=11.0`** is required — llama.cpp uses `std::filesystem` which needs macOS 11+.

**`Info.plist`** must include:
- `NSMicrophoneUsageDescription` — without it, `.app` launched from Spotlight gets silent audio buffers
- `LSUIElement = true` — hides from Dock and app switcher

---

## 14. Hard-Won Lessons

### The ggml Symbol Collision

whisper-rs and llama-cpp-sys both vendor ggml. On macOS, llama-cpp-sys unconditionally compiles the Metal backend via cmake auto-detection. When both are linked, duplicate Metal symbols cause SIGABRT during backend registry initialization. **Solution:** llama-server runs out-of-process.

### macOS Audio Devices Don't Support 16kHz

MacBook mics run at 48kHz. Requesting 16kHz via cpal config fails. Always use `device.default_input_config()` and resample in the cpal callback.

### cpal Stream is !Send

Cannot hold a cpal `Stream` across `.await` points on tokio's multi-threaded runtime. The audio pipeline needs its own dedicated thread with a `current_thread` runtime.

### enigo Crashes From Non-Main Thread

`TSMGetInputSourceProperty` must run on the main dispatch queue. Replaced with osascript, then replaced again with CGEvent API (osascript broke from Spotlight launches).

### osascript Breaks From Spotlight

When launched from Spotlight, the app has a different identity than when launched from terminal. osascript's `keystroke` command gets `error 1002: not allowed to send keystrokes`. CGEvent API works because the Accessibility permission is tied to the `.app` bundle directly.

### Downloaded Binaries Need xattr Stripping

macOS adds `com.apple.quarantine` and `com.apple.provenance` to downloaded files. The llama-server binary won't execute until these are stripped with `xattr -c`.

### llama-server Needs Its Dylibs

The pre-built llama-server from GitHub releases dynamically links 8 dylibs (libggml-*.dylib, libllama.dylib, libmtmd.dylib). Must extract the entire tarball contents, not just the binary.

### Orphaned llama-server Survives App Crashes

`Drop` impl kills the child process on normal exit, but `kill -9` or crash skips Drop. On startup, kill any process on port 8384 before spawning a new server.

### AGC Breaks VAD

Automatic Gain Control was added then reverted — it boosts background noise above the energy VAD threshold, causing the chunker to never finalize segments (it thinks everything is speech).

### Orchestrator Must Outlive Audio Thread

In walkie-talkie mode, the audio thread stops on key release, then the chunker flushes the final segment. If the orchestrator runs on the audio thread's runtime, `spawn_blocking` tasks get killed when the thread stops. The orchestrator must run on Tauri's multi-threaded runtime.

---

## 15. Tech Stack

| Layer | Technology | Why |
|---|---|---|
| App framework | Tauri 2 + Svelte | Native webview, <50MB idle RAM, system tray, global shortcuts |
| Audio capture | cpal 0.15 | Cross-platform, low-latency |
| VAD | Energy-based (Silero fallback) | No external dylib dependency |
| ASR | whisper-rs 0.13 (whisper.cpp) | Fastest local Whisper, CPU ~300ms |
| LLM | llama-server (llama.cpp b8123) | Out-of-process, Metal GPU, auto-downloaded |
| Text injection | core-graphics CGEvent + arboard | Works from any thread, any launch context |
| Database | rusqlite 0.31 (bundled SQLite) | Zero-config, embedded |
| HTTP client | ureq 3 | llama-server API + model downloads |
| Shortcuts | tauri-plugin-global-shortcut 2 | Press + release events for walkie-talkie |

---

## 16. What's Next

| Priority | Feature | Notes |
|----------|---------|-------|
| P0 | Re-enable Metal for whisper | Safe now that llama-cpp is out-of-process. Would cut ASR from 300ms to ~100ms |
| P1 | Streaming token injection | Inject sentence-by-sentence as LLM generates, not batch |
| P1 | Graceful degradation | Dictate without polish if LLM not ready; show text in pill if accessibility not granted |
| P2 | Download resume | Resume partial downloads instead of restarting |
| P2 | SHA256 checksum verification | Belt-and-suspenders for model integrity |
| P2 | Correction learning | Detect user edits after injection, auto-add to personal dictionary |
| P3 | Noise suppression | RNNoise via nnnoiseless crate |
| P3 | Windows support | UIAutomation + clipboard fallback |
| P3 | Whisper mode | Lower VAD threshold for quiet speech |
